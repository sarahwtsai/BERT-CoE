{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Q&A Datasets\n",
    "This notebook will download and prepare Q&A datasets for training a feed forward network/routing network. The following datasets will be prepared: \n",
    "- ARC AI2 Reasoning Challenge: 7,787 grade-school science questions\n",
    "- OpenBookQA: 5,957 science and reasoning questions\n",
    "- MathQA: ~37,000 math word problems\n",
    "- CommonsenseQA: 12,247 general reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC AI2 Reasoning Challenge\n",
    "\n",
    "https://huggingface.co/datasets/allenai/ai2_arc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 7787/7787 [00:00<00:00, 318178.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_arc1 = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")\n",
    "ds_arc2 = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")\n",
    "\n",
    "# Add the \"source\" field to each example\n",
    "def add_source_field(example, source_name):\n",
    "    example['source'] = source_name\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to each split in both datasets\n",
    "ds_arc1_train = ds_arc1['train'].map(lambda x: add_source_field(x, \"ARC-Challenge\"))\n",
    "ds_arc1_test = ds_arc1['test'].map(lambda x: add_source_field(x, \"ARC-Challenge\"))\n",
    "ds_arc1_validation = ds_arc1['validation'].map(lambda x: add_source_field(x, \"ARC-Challenge\"))\n",
    "\n",
    "ds_arc2_train = ds_arc2['train'].map(lambda x: add_source_field(x, \"ARC-Easy\"))\n",
    "ds_arc2_test = ds_arc2['test'].map(lambda x: add_source_field(x, \"ARC-Easy\"))\n",
    "ds_arc2_validation = ds_arc2['validation'].map(lambda x: add_source_field(x, \"ARC-Easy\"))\n",
    "\n",
    "# Concatenate all dataset splits\n",
    "arc_train_set = concatenate_datasets([ds_arc1_train, ds_arc1_test, ds_arc1_validation,\n",
    "                                      ds_arc2_train, ds_arc2_test, ds_arc2_validation])\n",
    "\n",
    "# Save the new dataset to disk\n",
    "arc_train_set.save_to_disk(\"../datasets/ARC_AI2.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBookQA\n",
    "\n",
    "https://huggingface.co/datasets/allenai/openbookqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 11914/11914 [00:00<00:00, 401198.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_ob1 = load_dataset(\"allenai/openbookqa\", \"additional\")\n",
    "ds_ob2 = load_dataset(\"allenai/openbookqa\", \"main\")\n",
    "\n",
    "# Add the \"source\" field to each example\n",
    "def add_source_field(example, source_name):\n",
    "    example['source'] = source_name\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to each split in both datasets\n",
    "ds_ob1_train = ds_ob1['train'].map(lambda x: add_source_field(x, \"OpenBook-Additional\"))\n",
    "ds_ob1_test = ds_ob1['test'].map(lambda x: add_source_field(x, \"OpenBook-Additional\"))\n",
    "ds_ob1_validation = ds_ob1['validation'].map(lambda x: add_source_field(x, \"OpenBook-Additional\"))\n",
    "\n",
    "ds_ob2_train = ds_ob2['train'].map(lambda x: add_source_field(x, \"OpenBook-Main\"))\n",
    "ds_ob2_test = ds_ob2['test'].map(lambda x: add_source_field(x, \"OpenBook-Mai\"))\n",
    "ds_ob2_validation = ds_ob2['validation'].map(lambda x: add_source_field(x, \"OpenBook-Mai\"))\n",
    "\n",
    "# Concatenate all dataset splits\n",
    "ob_train_set = concatenate_datasets([ds_ob1_train, ds_ob1_test, ds_ob1_validation,\n",
    "                                     ds_ob2_train, ds_ob2_test, ds_ob2_validation])\n",
    "\n",
    "# Save the new dataset to disk\n",
    "ob_train_set.save_to_disk(\"../datasets/OpenBook.hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MathQA\n",
    "\n",
    "https://math-qa.github.io/math-QA/\n",
    "\n",
    "JSON files will needed to be converted to HF format with the same labels as the other Q&A datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in MathQA: 37901\n"
     ]
    }
   ],
   "source": [
    "json_dir = \"../datasets/MathQA\"\n",
    "\n",
    "# List to store combined JSON data\n",
    "all_data = []\n",
    "\n",
    "# Loop through all JSON files in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith(\".json\"):  # Ensure it's a JSON file\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        \n",
    "        # Load the JSON file and append its content to all_data\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)  # Each file is a list of dictionaries\n",
    "            all_data.extend(data)  # Concatenate lists\n",
    "\n",
    "print('Number of examples in MathQA:', len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 37901/37901 [00:00<00:00, 439431.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Modify example problems to match labels and formatting for other datasets\n",
    "# Function to transform options into the correct format\n",
    "def parse_options(entry):\n",
    "    # Extract choices using regex\n",
    "    choices = re.findall(r\"[a-e] \\) (.*?)(?= , [a-e] \\) |$)\", entry)\n",
    "    return {\"text\": choices, \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"]}  \n",
    "       \n",
    "\n",
    "# Function to update keys and values\n",
    "def update_entry(entry):\n",
    "    return {\n",
    "        \"question\": entry[\"Problem\"],  \n",
    "        \"rationale\": entry[\"Rationale\"],\n",
    "        \"choices\": parse_options(entry[\"options\"]),  \n",
    "        \"correct\": entry[\"correct\"].upper(),\n",
    "        \"annotatedFormula\": entry[\"annotated_formula\"],\n",
    "        \"linearFormula\": entry[\"linear_formula\"],\n",
    "        \"category\": entry[\"category\"],\n",
    "        \"source\": \"MathQA\"\n",
    "    }\n",
    "\n",
    "# Apply transformation\n",
    "updated_data = [update_entry(entry) for entry in all_data]\n",
    "ds_mq = Dataset.from_list(updated_data)\n",
    "ds_mq.save_to_disk(\"../datasets/MathQA.hf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommonsenseQA\n",
    "\n",
    "https://huggingface.co/datasets/tau/commonsense_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9741/9741 [00:00<00:00, 12150.95 examples/s]\n",
      "Map: 100%|██████████| 1221/1221 [00:00<00:00, 13205.82 examples/s]\n",
      "Map: 100%|██████████| 1140/1140 [00:00<00:00, 13420.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 12102/12102 [00:00<00:00, 570446.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_cs = load_dataset(\"tau/commonsense_qa\")\n",
    "\n",
    "# Add the \"source\" field to each example\n",
    "def add_source_field(example, source_name):\n",
    "    example['source'] = source_name\n",
    "    return example\n",
    "\n",
    "# Apply the transformation to each split in the dataset\n",
    "ds_cs_train = ds_cs['train'].map(lambda x: add_source_field(x, \"Commonsense\"))\n",
    "ds_cs_validation = ds_cs['validation'].map(lambda x: add_source_field(x, \"Commonsense\"))\n",
    "ds_cs_test = ds_cs['test'].map(lambda x: add_source_field(x, \"Commonsense\"))\n",
    "\n",
    "# Concatenate all dataset splits\n",
    "cs_train_set = concatenate_datasets([ds_cs_train, ds_cs_validation, ds_cs_test])\n",
    "\n",
    "# Save the new dataset to disk\n",
    "cs_train_set.save_to_disk(\"../datasets/Commonsense.hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
